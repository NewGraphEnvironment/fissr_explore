--- 
title: "Linking Fish Density to Habitat Characteristics"
author: |
 |
 | Prepared for

 |
 |
 | Prepared by


date: |
 |
 | Version 0.0.1
 | `r format(Sys.Date(), "%Y-%m-%d")`
toc-title: Table of Contents
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
          includes:
            in_header: header.html
nocite: |

documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: no
github-repo: rstudio/bookdown-demo
description: "My Description."

---


```{r setup, include = TRUE, echo =FALSE, message=FALSE, warning=FALSE}
gitbook_on <- TRUE
# gitbook_on <- FALSE  ##we just need turn  this on and off to switch between gitbook and pdf via paged.js


knitr::opts_chunk$set(echo=T, message=FALSE, warning=FALSE, dpi=60, out.width = "100%")
options(scipen=999)
options(knitr.kable.NA = '--') #'--'
options(knitr.kable.NAN = '--')



```

```{r settings-gitbook, eval= gitbook_on}
photo_width <- "100%"
font_set <- 11

```

```{r settings-paged-html, eval= identical(gitbook_on, FALSE)}
photo_width <- "80%"
font_set <- 9
```


```{r}
source('R/private_info.R')
source('R/packages.R')
source('R/functions.R')
```


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# Acknowledgement {.front-matter .unnumbered}


Modern civilization has a long journey ahead to acknowledge and address the historic and ongoing impacts of colonialism that have resulted in harm to  ecosystems on Turtle Island.  That harm naturally extends to the cultures and livelihoods of those that have lived in harmony with the land for many thousands of years.



```{js, logo-header, echo = FALSE, eval= T}
title=document.getElementById('header');
title.innerHTML = '<img src="fig/collaboration.jpg" alt="NewGraph">' + title.innerHTML
```


<!--chapter:end:index.Rmd-->



# Big Picture
Big picture here is to build evidence based approach to the parameters used to model the best habitat for individual fish species to help make decisions around where to invest in aquatic restoration activities (along with lots of others factors of course).  [`bcfishpass`](https://github.com/smnorris/bcfishpass/tree/main/02_model/parameters_newgraph) will except habitat parameters that allow us to model the quantity and quality of potential spawning and rearing habitat upstream of culvert locations. How do we come up with those parameters? 

<br>

Using the electrofishing data from British Columbia databases (supplied by [Robin Munro](https://dir.gov.bc.ca/gtds.cgi?esearch=&updateRequest=&view=detailed&sortBy=name&for=people&attribute=display+name&matchMethod=is&searchString=Robin+Munro&objectId=125664) from the BC Ministry of Environment) this work intends to tie fish density to habitat characteristics. Gradient is easy in the freshwater atlas, and discharge data is available for some watersheds from [PCIC](https://www.pacificclimate.org/data/gridded-hydrologic-model-output).  We are developing methods to estimate [channel width](https://github.com/smnorris/bcfishpass/tree/main/01_prep/habitat/02_channel_width) using [Bayesian modelling](https://github.com/NewGraphEnvironment/fish_passage_bulkley_2020_reporting/blob/master/docs/channel-width-21.pdf) of watershed size, precipitation and potentially other factors. We hope to step back to using parameters that have fed the PCIC discharge data to build discharge estimates raw when time and funding allows. 

<br>

[Joe Thorley](https://github.com/poissonconsulting) had a great point when he said that though we are starting with BC data we will keep our minds open to ways of bringing in datasets from elsewhere in the world in the long term. 

<br>

Through this work and numerous other initiatives [Simon Norris](https://github.com/smnorris) has been evolving [fwapg](https://github.com/smnorris/fwapg).fwapg is leveraged with [fwapgr](https://github.com/poissonconsulting/fwapgr) to provide an R Client to the database and expand the tool's functionality. To gather stream segment and [watershed characteristics](https://github.com/smnorris/fissr_explore/tree/master/scripts) related to the electrofishing density points that were shared with us by the province these tools are expanding the information in the dataset. The resulting outputs are constantly evolving with initial versions undergoing defensible magic in [fissr-explore-21](https://github.com/poissonconsulting/fissr-explore-21).



<!--chapter:end:0100-intro.Rmd-->



# Explore FWA Streams
Purpose of this section was to explore the lengths and frequencies of double lined streams as there is a channel width predictor model `channel-width-21`.  Result of this work was that Simon has sampled each double line stream 30 times and kept the average and standard deviation - regarless of segment length [`bcfishpass`](https://github.com/smnorris/bcfishpass/tree/main/01_prep/habitat/02_channel_width).


Load the double line streams used in channel_width_mapped.  In order to facilitate reproducability lets save the streams in a sqlite and pull them back in.


```{r eval=FALSE}


##gconnect to database
conn <- DBI::dbConnect(
  RPostgres::Postgres(),
  dbname = dbname_wsl,
  host = host_wsl,
  port = port_wsl,
  user = user_wsl,
  password = password_wsl
)



##get the segments that have a centreline (double line streams)
q <- ("SELECT linear_feature_id, ceil(upstream_route_measure)::integer - floor(downstream_route_measure)::integer AS len,
      watershed_group_code
      FROM whse_basemapping.fwa_stream_networks_sp 
      WHERE edge_type = 1250")

streams <- sf::st_read(conn, query = q) 



##  the streams in a sqlite and pull them back in.
##########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#########################
###change this up so that it matches your version of the ahred dropbox!




mydb <- DBI::dbConnect(RSQLite::SQLite(), paste0(shared_dropbox_dir, "fiss/fissr_explore.sqlite"))


conn <- readwritesqlite::rws_connect(paste0(shared_dropbox_dir, "fiss/fissr_explore.sqlite"))
readwritesqlite::rws_write(streams, exists = F, delete = TRUE,
          conn = conn, x_name = "whse_basemapping.fwa_stream_networks_sp")
readwritesqlite::rws_list_tables(conn)
readwritesqlite::rws_disconnect(conn)



```

```{r}



shared_dropbox_dir <- "C:/Users/al/Dropbox/New Graph/"
```


Load the streams from sqlite.  The table I pull back in after saving as `sqlite` has `linear_feature_id` as integer vs int64 as it was pulled from `postgres`. No issues for this exercise though...

```{r}
conn <- readwritesqlite::rws_connect(paste0(shared_dropbox_dir, "fiss/fissr_explore.sqlite"))
streams <- readwritesqlite::rws_read_table("whse_basemapping.fwa_stream_networks_sp", conn = conn)
readwritesqlite::rws_disconnect(conn)
```

```{r}
summary(streams$len)

```


Lets look at distribution of lengths
```{r}
ggplot(select(streams, len), aes(x=len)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "length", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)

```


```{r}
##segments less than 1000m long
ggplot(select(streams, len) %>% filter(len < 1000), aes(x=len)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "length", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)
```

```{r}
##segments less than 250m long
ggplot(select(streams, len) %>% filter(len < 250), aes(x=len)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "length", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)
```

```{r}
##how many streams are segments are smaller than 50m
streams %>% 
  filter(len > 1000) %>% 
  count()
```

```{r}
streams_equal <- streams %>% 
  mutate(cut = cut_number(len, n = 5)) %>% 
  group_by(cut) %>% 
  summarise(n = n())

streams_equal




```

```{r}
streams_interval <- streams %>% 
  mutate(cut = cut_interval(len, n = 10)) %>% 
  group_by(cut) %>% 
  summarise(n = n())

streams_interval
```

```{r}
streams_length <- streams %>%
  mutate(cut =
           case_when(
             len < 100 ~ '0_100',
             len >= 100 & len < 500 ~ '0100_500',
             len >= 500 & len < 999 ~ '0500_1000',
             len >=999  ~ '1000+')) 

streams_length_sum <- streams_length %>% 
  group_by(cut) %>% 
  summarise(n = n())

streams_length_sum
```


```{r}
ggplot(data = streams_length_sum, mapping = aes(x=cut, y = n)) + 
  geom_bar(stat = "identity") + 
  # ylim(1000, 20000)+
  # labs(x='test') +
  ggdark::dark_theme_bw(base_size = 11)
```


```{r}
ggplot(streams_length, aes(x=len)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "length", y = "#") +
  facet_wrap(~cut, ncol = 2, scales = "free")+
  ggdark::dark_theme_bw(base_size = 11)
```


<!--chapter:end:0110-fwa.Rmd-->



# Explore Data Exported from FISS
Purpose of this section is to explore the species density data provided in csv format from the province.  We would like to be able to tie density of fish species to habitat characteristics including gradient, channel size, discharge, elevation, forest cover, etc.


Load the data from the dropbox


```{r}
files_to_read <- list.files(paste0(shared_dropbox_dir,"fiss/2020-11-16/"), full.names = T)
files_names <- list.files(paste0(shared_dropbox_dir,"fiss/2020-11-16/"), full.names = F) %>% tools::file_path_sans_ext()


##mini function to change column names to lower case
names_to_lower <- function(dat){
  dat %>% 
  purrr::set_names(nm = stringr::str_to_lower(names(dat)))
}


d <- files_to_read %>% 
  map(readr::read_csv) %>% 
  map(names_to_lower) %>% 
  purrr::set_names(nm = files_names)

```


Make a table that has all the info we want in one place for the summarized fish

```{r}
d_sum_raw <- left_join(
  d %>% pluck('counts'),
  d %>% pluck('visits'),
  by = 'key'
  ) %>% 
  left_join(
    .,
    d %>% pluck('habitat'),
    by = 'key'
  ) %>% 
##clean it up and grab a density
  filter(!is.na(fishing_area_length) & 
           fishing_area_length > 0 &
           !is.na(fishing_area_width) &
           fishing_area_width > 0 &
           !is.na(utm_easting) &
           !is.na(utm_northing) &
           utm_zone > 0 &
           #For the sake of a first run lets keep only the first passes from sites that have multiple passes
           haul_or_pass == 1 &
           species_code %in% c('RB', 'BT', 'GR', 'CH', 'CO', 'SK', 'WCT', 'ST', 'NFC')) %>%
  mutate(density_100m = number_caught/(fishing_area_length * fishing_area_width) * 100)
  
```




Lets review how many occurrences are from the same site - not sure it matters though.
```{r}
d_same_site <- d_sum_raw %>% 
  group_by(across(data_set:agncy_id)) %>% 
  summarise(n = n()) %>% 
  filter(n > 1)
  
```

<br>

Lets have a look at what we have by species.
```{r}
d_raw_by_sp <- d_sum_raw %>% 
  group_by(species_code) %>% 
  mutate(area = fishing_area_length * fishing_area_width) %>% 
  summarise(n = n(), 
            dens_min = min(density_100m, na.rm = T),
            dens_max = max(density_100m, na.rm = T),
            dens_med = median(density_100m, na.rm = T),
            area_min = min(area, na.rm = T),
            area_max = max(area, na.rm = T),
            area_med = median(area, na.rm = T),
            len_min = min(fishing_area_length, na.rm = T),
            len_max = max(fishing_area_length, na.rm = T),
            len_med = median(fishing_area_length, na.rm = T),
            wid_min = min(fishing_area_width, na.rm = T),
            wid_max = max(fishing_area_width, na.rm = T),
            wid_med = median(fishing_area_width, na.rm = T))

d_raw_by_sp
```


```{r}
ggplot(select(d_sum_raw, fishing_area_length), aes(x=fishing_area_length)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "fishing_area_length", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)

```

```{r}
ggplot(select(d_sum_raw, fishing_area_width), aes(x=fishing_area_width)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "fishing_area_width", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)

```


Lets trim out our strange numbers for the site sizes


```{r}
d_sum <- left_join(
  d %>% pluck('counts'),
  d %>% pluck('visits'),
  by = 'key'
  ) %>% 
  left_join(
    .,
    d %>% pluck('habitat'),
    by = 'key'
  ) %>% 
##clean it up and grab a density
  filter(!is.na(fishing_area_length) & 
           fishing_area_length > 0.9 &
           fishing_area_length < 500.1 &
           !is.na(fishing_area_width) &
           fishing_area_width > 0.5 &
           fishing_area_width < 25.1 &
           !is.na(utm_easting) &
           !is.na(utm_northing) &
           utm_zone > 0 &
           #For the sake of a first run lets keep only the first passes from sites that have multiple passes
           haul_or_pass == 1 &
           species_code %in% c('RB', 'BT', 'GR', 'CH', 'CO', 'SK', 'WCT', 'ST', 'NFC')) %>%
  mutate(density_100m = number_caught/(fishing_area_length * fishing_area_width) * 100)


##have another look at it
d_by_sp <- d_sum %>% 
  group_by(species_code) %>% 
  mutate(area = fishing_area_length * fishing_area_width) %>% 
  summarise(n = n(), 
            dens_min = min(density_100m, na.rm = T),
            dens_max = max(density_100m, na.rm = T),
            dens_med = median(density_100m, na.rm = T),
            area_min = min(area, na.rm = T),
            area_max = max(area, na.rm = T),
            area_med = median(area, na.rm = T),
            len_min = min(fishing_area_length, na.rm = T),
            len_max = max(fishing_area_length, na.rm = T),
            len_med = median(fishing_area_length, na.rm = T),
            wid_min = min(fishing_area_width, na.rm = T),
            wid_max = max(fishing_area_width, na.rm = T),
            wid_med = median(fishing_area_width, na.rm = T))

d_by_sp  
```



```{r}
ggplot(select(d_sum, fishing_area_length), aes(x=fishing_area_length)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "fishing_area_length", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)

```

```{r}
ggplot(select(d_sum, fishing_area_width), aes(x=fishing_area_width)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "fishing_area_width", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)

```

<br>

What do the small sites look like?  Should we keep them?

```{r}

length_cut <- 10
width_cut <- 1

sites_small <- d_sum %>%
  filter(fishing_area_length < length_cut |
           fishing_area_width < width_cut) %>% 
  mutate(wettedwidth_ave = rowMeans(select(., starts_with("wetted")), na.rm = TRUE)) %>% 
  select(fishing_area_length, fishing_area_width, wettedwidth_ave)

sites_small
```

<br>

These are small sites that have associated wetted widths

```{r}
sites_small %>% 
  filter(!is.na(wettedwidth_ave))
```

```{r}
sites_small_len <- sites_small %>% 
  filter(fishing_area_length < length_cut)

sites_small_len
```
<br>

Distribution of sites with small lengths 
```{r}
ggplot(select(sites_small_len, fishing_area_length), aes(x=fishing_area_length)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "fishing_area_length", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)

```


<br>

Sites with small widths

```{r}
sites_small_wid <- sites_small %>% 
  filter(fishing_area_width < width_cut)

sites_small_wid
```

<br>

Distribution of sites with small widths.  Are these sites where it was a presence/absence test so maybe not suitable for density modelling? Don't know...
```{r}
ggplot(select(sites_small_wid, fishing_area_width), aes(x=fishing_area_width)) +
  geom_histogram(position="identity", size = 0.75)+
  labs(x = "fishing_area_width", y = "#") +
  ggdark::dark_theme_bw(base_size = 11)

```





<!--chapter:end:0120-fiss.Rmd-->



# Make FISS data spatial
We need in spatial format to tie to the FWA so lets put everything in albers and add the X and Y to the dataframe

```{r}
d_sum_albers <- d_sum %>% 
  dplyr::group_split(utm_zone) %>% 
  map(utm_to_coord) %>% 
  bind_rows()
```

<br>

Burn to a csv so that Simon can access and join to habitat characteristics

```{r}
d_sum_albers %>% readr::write_csv(file = paste0(getwd(), '/data/fiss_density.csv'))
```

Next steps?:

 * While Joe and Nadine tweak the channel-width-21 analysis and have a look at what data from here they think we should keep or throw away...
 * Simon maybe burns dataframe into postgres that represents the X and Y locations of these sites and ties the locations of the sites to stream segments then pulls out everything he can about the watershed upstream (ex. elevation, watershed size, channel width, discharge, BEC zone, precip, etc ).  Future considerations include perhaps forest cover, geology, channel confinement, [Variable Infiltration Capacity (VIC-GL) model stuff](https://www.pacificclimate.org/data/gridded-hydrologic-model-output))
 * We should QA how well the sites tie to streams and find a way to filter out (or correct) bad matches if possible.
 * Feed results back to Joe and Nadine to see what we can gleen from the data.

<!--chapter:end:0130-fiss_spatial.Rmd-->

