[["index.html", "Linking Fish Density to Habitat Characteristics Acknowledgement", " Linking Fish Density to Habitat Characteristics Prepared for Prepared by Version 0.0.1 2021-12-05 Acknowledgement Modern civilization has a long journey ahead to acknowledge and address the historic and ongoing impacts of colonialism that have resulted in harm to ecosystems on Turtle Island. That harm naturally extends to the cultures and livelihoods of those that have lived in harmony with the land for many thousands of years. "],["big-picture.html", "1 Big Picture", " 1 Big Picture Big picture here is to build evidence based approach to the parameters used to model the best habitat for individual fish species to help make decisions around where to invest in aquatic restoration activities (along with lots of others factors of course). bcfishpass will except habitat parameters that allow us to model the quantity and quality of potential spawning and rearing habitat upstream of culvert locations. How do we come up with those parameters? Using the electrofishing data from British Columbia databases (supplied by Robin Munro from the BC Ministry of Environment) this work intends to tie fish density to habitat characteristics. Gradient is easy in the freshwater atlas, and discharge data is available for some watersheds from PCIC. We are developing methods to estimate channel width using Bayesian modelling of watershed size, precipitation and potentially other factors. We hope to step back to using parameters that have fed the PCIC discharge data to build discharge estimates raw when time and funding allows. Joe Thorley had a great point when he said that though we are starting with BC data we will keep our minds open to ways of bringing in datasets from elsewhere in the world in the long term. Through this work and numerous other initiatives Simon Norris has been evolving fwapg. fwapg is leveraged with fwapgr to provide an R Client to the database and expand the tools functionality. To gather stream segment and watershed characteristics related to the electrofishing density points that were shared with us by the province these tools are expanding the information in the dataset. The resulting outputs are constantly evolving with initial versions undergoing defensible magic in fissr-explore-21. "],["explore-fwa-streams.html", "2 Explore FWA Streams", " 2 Explore FWA Streams Purpose of this section was to explore the lengths and frequencies of double lined streams as there is a channel width predictor model channel-width-21. Result of this work was that Simon has sampled each double line stream 30 times and kept the average and standard deviation - regarless of segment length bcfishpass. Load the double line streams used in channel_width_mapped. In order to facilitate reproducability lets save the streams in a sqlite and pull them back in. ##gconnect to database conn &lt;- DBI::dbConnect( RPostgres::Postgres(), dbname = dbname_wsl, host = host_wsl, port = port_wsl, user = user_wsl, password = password_wsl ) ##get the segments that have a centreline (double line streams) q &lt;- (&quot;SELECT linear_feature_id, ceil(upstream_route_measure)::integer - floor(downstream_route_measure)::integer AS len, watershed_group_code FROM whse_basemapping.fwa_stream_networks_sp WHERE edge_type = 1250&quot;) streams &lt;- sf::st_read(conn, query = q) ## the streams in a sqlite and pull them back in. ##########################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!######################### ###change this up so that it matches your version of the ahred dropbox! mydb &lt;- DBI::dbConnect(RSQLite::SQLite(), paste0(shared_dropbox_dir, &quot;fiss/fissr_explore.sqlite&quot;)) conn &lt;- readwritesqlite::rws_connect(paste0(shared_dropbox_dir, &quot;fiss/fissr_explore.sqlite&quot;)) readwritesqlite::rws_write(streams, exists = F, delete = TRUE, conn = conn, x_name = &quot;whse_basemapping.fwa_stream_networks_sp&quot;) readwritesqlite::rws_list_tables(conn) readwritesqlite::rws_disconnect(conn) shared_dropbox_dir &lt;- &quot;C:/Users/al/Dropbox/New Graph/&quot; Load the streams from sqlite. The table I pull back in after saving as sqlite has linear_feature_id as integer vs int64 as it was pulled from postgres. No issues for this exercise though conn &lt;- readwritesqlite::rws_connect(paste0(shared_dropbox_dir, &quot;fiss/fissr_explore.sqlite&quot;)) streams &lt;- readwritesqlite::rws_read_table(&quot;whse_basemapping.fwa_stream_networks_sp&quot;, conn = conn) readwritesqlite::rws_disconnect(conn) summary(streams$len) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 86.0 181.0 294.5 362.0 9253.0 Lets look at distribution of lengths ggplot(select(streams, len), aes(x=len)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;length&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) ##segments less than 1000m long ggplot(select(streams, len) %&gt;% filter(len &lt; 1000), aes(x=len)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;length&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) ##segments less than 250m long ggplot(select(streams, len) %&gt;% filter(len &lt; 250), aes(x=len)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;length&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) ##how many streams are segments are smaller than 50m streams %&gt;% filter(len &gt; 1000) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 8286 streams_equal &lt;- streams %&gt;% mutate(cut = cut_number(len, n = 5)) %&gt;% group_by(cut) %&gt;% summarise(n = n()) streams_equal ## # A tibble: 5 x 2 ## cut n ## &lt;fct&gt; &lt;int&gt; ## 1 [1,70] 38500 ## 2 (70,138] 38232 ## 3 (138,235] 37768 ## 4 (235,427] 38064 ## 5 (427,9.25e+03] 38099 streams_interval &lt;- streams %&gt;% mutate(cut = cut_interval(len, n = 10)) %&gt;% group_by(cut) %&gt;% summarise(n = n()) streams_interval ## # A tibble: 10 x 2 ## cut n ## &lt;fct&gt; &lt;int&gt; ## 1 [1,926] 180821 ## 2 (926,1.85e+03] 8096 ## 3 (1.85e+03,2.78e+03] 1296 ## 4 (2.78e+03,3.7e+03] 294 ## 5 (3.7e+03,4.63e+03] 90 ## 6 (4.63e+03,5.55e+03] 33 ## 7 (5.55e+03,6.48e+03] 21 ## 8 (6.48e+03,7.4e+03] 7 ## 9 (7.4e+03,8.33e+03] 4 ## 10 (8.33e+03,9.25e+03] 1 streams_length &lt;- streams %&gt;% mutate(cut = case_when( len &lt; 100 ~ &#39;0_100&#39;, len &gt;= 100 &amp; len &lt; 500 ~ &#39;0100_500&#39;, len &gt;= 500 &amp; len &lt; 999 ~ &#39;0500_1000&#39;, len &gt;=999 ~ &#39;1000+&#39;)) streams_length_sum &lt;- streams_length %&gt;% group_by(cut) %&gt;% summarise(n = n()) streams_length_sum ## # A tibble: 4 x 2 ## cut n ## &lt;chr&gt; &lt;int&gt; ## 1 0_100 55821 ## 2 0100_500 104540 ## 3 0500_1000 21988 ## 4 1000+ 8314 ggplot(data = streams_length_sum, mapping = aes(x=cut, y = n)) + geom_bar(stat = &quot;identity&quot;) + # ylim(1000, 20000)+ # labs(x=&#39;test&#39;) + ggdark::dark_theme_bw(base_size = 11) ggplot(streams_length, aes(x=len)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;length&quot;, y = &quot;#&quot;) + facet_wrap(~cut, ncol = 2, scales = &quot;free&quot;)+ ggdark::dark_theme_bw(base_size = 11) "],["explore-data-exported-from-fiss.html", "3 Explore Data Exported from FISS", " 3 Explore Data Exported from FISS Purpose of this section is to explore the species density data provided in csv format from the province. We would like to be able to tie density of fish species to habitat characteristics including gradient, channel size, discharge, elevation, forest cover, etc. Load the data from the dropbox files_to_read &lt;- list.files(paste0(shared_dropbox_dir,&quot;fiss/2020-11-16/&quot;), full.names = T) files_names &lt;- list.files(paste0(shared_dropbox_dir,&quot;fiss/2020-11-16/&quot;), full.names = F) %&gt;% tools::file_path_sans_ext() ##mini function to change column names to lower case names_to_lower &lt;- function(dat){ dat %&gt;% purrr::set_names(nm = stringr::str_to_lower(names(dat))) } d &lt;- files_to_read %&gt;% map(readr::read_csv) %&gt;% map(names_to_lower) %&gt;% purrr::set_names(nm = files_names) Make a table that has all the info we want in one place for the summarized fish d_sum_raw &lt;- left_join( d %&gt;% pluck(&#39;counts&#39;), d %&gt;% pluck(&#39;visits&#39;), by = &#39;key&#39; ) %&gt;% left_join( ., d %&gt;% pluck(&#39;habitat&#39;), by = &#39;key&#39; ) %&gt;% ##clean it up and grab a density filter(!is.na(fishing_area_length) &amp; fishing_area_length &gt; 0 &amp; !is.na(fishing_area_width) &amp; fishing_area_width &gt; 0 &amp; !is.na(utm_easting) &amp; !is.na(utm_northing) &amp; utm_zone &gt; 0 &amp; #For the sake of a first run lets keep only the first passes from sites that have multiple passes haul_or_pass == 1 &amp; species_code %in% c(&#39;RB&#39;, &#39;BT&#39;, &#39;GR&#39;, &#39;CH&#39;, &#39;CO&#39;, &#39;SK&#39;, &#39;WCT&#39;, &#39;ST&#39;, &#39;NFC&#39;)) %&gt;% mutate(density_100m = number_caught/(fishing_area_length * fishing_area_width) * 100) Lets review how many occurrences are from the same site - not sure it matters though. d_same_site &lt;- d_sum_raw %&gt;% group_by(across(data_set:agncy_id)) %&gt;% summarise(n = n()) %&gt;% filter(n &gt; 1) Lets have a look at what we have by species. d_raw_by_sp &lt;- d_sum_raw %&gt;% group_by(species_code) %&gt;% mutate(area = fishing_area_length * fishing_area_width) %&gt;% summarise(n = n(), dens_min = min(density_100m, na.rm = T), dens_max = max(density_100m, na.rm = T), dens_med = median(density_100m, na.rm = T), area_min = min(area, na.rm = T), area_max = max(area, na.rm = T), area_med = median(area, na.rm = T), len_min = min(fishing_area_length, na.rm = T), len_max = max(fishing_area_length, na.rm = T), len_med = median(fishing_area_length, na.rm = T), wid_min = min(fishing_area_width, na.rm = T), wid_max = max(fishing_area_width, na.rm = T), wid_med = median(fishing_area_width, na.rm = T)) d_raw_by_sp ## # A tibble: 9 x 14 ## species_code n dens_min dens_max dens_med area_min area_max area_med len_min len_max len_med wid_min wid_max wid_med ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BT 4398 0.00000500 200 0.571 1 99980001 390 1 9999 100 0.5 10000. 5 ## 2 CH 1303 0.000182 848. 2.5 1 549994. 240 1 1600 80 0.7 10000. 3 ## 3 CO 3393 0.00855 6292. 6 0.72 549994. 112 1 1950 30 0.3 10000. 3 ## 4 GR 770 0.00110 125 0.333 20 91200 600 5 3000 150 0.5 708 4 ## 5 NFC 23136 0 0 0 0.1 99998000. 200 0.5 10000. 110 0.1 10000. 1.5 ## 6 RB 14500 0 14700 1.33 1 99998000. 250 1 10000. 100 0.1 10000. 4 ## 7 SK 80 0.0182 103. 0.558 31 5500 240 16 1600 80 1 20 3 ## 8 ST 415 0.0286 382. 7.34 31.3 7500 104. 6 1000 17.5 1.3 61.8 6 ## 9 WCT 2240 0 310 1.2 2 7969920. 250 2 3443 100 0.5 10000. 4 ggplot(select(d_sum_raw, fishing_area_length), aes(x=fishing_area_length)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;fishing_area_length&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) ggplot(select(d_sum_raw, fishing_area_width), aes(x=fishing_area_width)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;fishing_area_width&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) Lets trim out our strange numbers for the site sizes d_sum &lt;- left_join( d %&gt;% pluck(&#39;counts&#39;), d %&gt;% pluck(&#39;visits&#39;), by = &#39;key&#39; ) %&gt;% left_join( ., d %&gt;% pluck(&#39;habitat&#39;), by = &#39;key&#39; ) %&gt;% ##clean it up and grab a density filter(!is.na(fishing_area_length) &amp; fishing_area_length &gt; 0.9 &amp; fishing_area_length &lt; 500.1 &amp; !is.na(fishing_area_width) &amp; fishing_area_width &gt; 0.5 &amp; fishing_area_width &lt; 25.1 &amp; !is.na(utm_easting) &amp; !is.na(utm_northing) &amp; utm_zone &gt; 0 &amp; #For the sake of a first run lets keep only the first passes from sites that have multiple passes haul_or_pass == 1 &amp; species_code %in% c(&#39;RB&#39;, &#39;BT&#39;, &#39;GR&#39;, &#39;CH&#39;, &#39;CO&#39;, &#39;SK&#39;, &#39;WCT&#39;, &#39;ST&#39;, &#39;NFC&#39;)) %&gt;% mutate(density_100m = number_caught/(fishing_area_length * fishing_area_width) * 100) ##have another look at it d_by_sp &lt;- d_sum %&gt;% group_by(species_code) %&gt;% mutate(area = fishing_area_length * fishing_area_width) %&gt;% summarise(n = n(), dens_min = min(density_100m, na.rm = T), dens_max = max(density_100m, na.rm = T), dens_med = median(density_100m, na.rm = T), area_min = min(area, na.rm = T), area_max = max(area, na.rm = T), area_med = median(area, na.rm = T), len_min = min(fishing_area_length, na.rm = T), len_max = max(fishing_area_length, na.rm = T), len_med = median(fishing_area_length, na.rm = T), wid_min = min(fishing_area_width, na.rm = T), wid_max = max(fishing_area_width, na.rm = T), wid_med = median(fishing_area_width, na.rm = T)) d_by_sp ## # A tibble: 9 x 14 ## species_code n dens_min dens_max dens_med area_min area_max area_med len_min len_max len_med wid_min wid_max wid_med ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BT 3956 0.01 200 0.667 1 10000 300 1 500 100 0.6 25 4.7 ## 2 CH 1282 0.0268 848. 2.60 1 8000 240 1 500 80 0.7 25 3 ## 3 CO 3298 0.0291 6292. 6.25 0.72 7500 108. 1 500 30 0.6 25 3 ## 4 GR 658 0.00833 125 0.447 20 12000 498. 5 500 150 0.7 25 3.3 ## 5 NFC 20100 0 0 0 1 12500 200 1 500 107 0.6 25 1.6 ## 6 RB 13299 0 14700 1.58 1 12000 220 1 500 100 0.6 25 3.9 ## 7 SK 79 0.0182 103. 0.556 31 5500 240 16 400 80 1 20 3 ## 8 ST 397 0.112 382. 8.4 31.3 5000 102 6 410 17.1 1.3 20.4 6 ## 9 WCT 2137 0 310 1.26 2 6460 230 2 500 100 0.6 25 4 ggplot(select(d_sum, fishing_area_length), aes(x=fishing_area_length)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;fishing_area_length&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) ggplot(select(d_sum, fishing_area_width), aes(x=fishing_area_width)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;fishing_area_width&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) What do the small sites look like? Should we keep them? length_cut &lt;- 10 width_cut &lt;- 1 sites_small &lt;- d_sum %&gt;% filter(fishing_area_length &lt; length_cut | fishing_area_width &lt; width_cut) %&gt;% mutate(wettedwidth_ave = rowMeans(select(., starts_with(&quot;wetted&quot;)), na.rm = TRUE)) %&gt;% select(fishing_area_length, fishing_area_width, wettedwidth_ave) sites_small ## # A tibble: 5,067 x 3 ## fishing_area_length fishing_area_width wettedwidth_ave ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8 4 NaN ## 2 8 4 NaN ## 3 5 2 NaN ## 4 5 5 NaN ## 5 100 0.8 NaN ## 6 200 0.9 NaN ## 7 200 0.7 NaN ## 8 200 0.8 NaN ## 9 200 0.7 NaN ## 10 200 0.9 NaN ## # ... with 5,057 more rows These are small sites that have associated wetted widths sites_small %&gt;% filter(!is.na(wettedwidth_ave)) ## # A tibble: 2,125 x 3 ## fishing_area_length fishing_area_width wettedwidth_ave ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 80 0.7 0.717 ## 2 67 0.7 0.643 ## 3 100 0.7 0.867 ## 4 100 0.8 0.843 ## 5 100 0.8 0.795 ## 6 100 0.6 0.8 ## 7 100 0.9 0.947 ## 8 200 0.8 0.783 ## 9 100 0.6 0.617 ## 10 100 0.9 0.867 ## # ... with 2,115 more rows sites_small_len &lt;- sites_small %&gt;% filter(fishing_area_length &lt; length_cut) sites_small_len ## # A tibble: 2,266 x 3 ## fishing_area_length fishing_area_width wettedwidth_ave ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8 4 NaN ## 2 8 4 NaN ## 3 5 2 NaN ## 4 5 5 NaN ## 5 1 1 NaN ## 6 3 1 NaN ## 7 4 7 NaN ## 8 7 3 NaN ## 9 7 3 NaN ## 10 9 8 NaN ## # ... with 2,256 more rows Distribution of sites with small lengths ggplot(select(sites_small_len, fishing_area_length), aes(x=fishing_area_length)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;fishing_area_length&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) Sites with small widths sites_small_wid &lt;- sites_small %&gt;% filter(fishing_area_width &lt; width_cut) sites_small_wid ## # A tibble: 2,815 x 3 ## fishing_area_length fishing_area_width wettedwidth_ave ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 0.8 NaN ## 2 200 0.9 NaN ## 3 200 0.7 NaN ## 4 200 0.8 NaN ## 5 200 0.7 NaN ## 6 200 0.9 NaN ## 7 200 0.8 NaN ## 8 250 0.6 NaN ## 9 80 0.7 0.717 ## 10 67 0.7 0.643 ## # ... with 2,805 more rows Distribution of sites with small widths. Are these sites where it was a presence/absence test so maybe not suitable for density modelling? Dont know ggplot(select(sites_small_wid, fishing_area_width), aes(x=fishing_area_width)) + geom_histogram(position=&quot;identity&quot;, size = 0.75)+ labs(x = &quot;fishing_area_width&quot;, y = &quot;#&quot;) + ggdark::dark_theme_bw(base_size = 11) "],["make-fiss-data-spatial.html", "4 Make FISS data spatial", " 4 Make FISS data spatial We need in spatial format to tie to the FWA so lets put everything in albers and add the X and Y to the dataframe d_sum_albers &lt;- d_sum %&gt;% dplyr::group_split(utm_zone) %&gt;% map(utm_to_coord) %&gt;% bind_rows() Burn to a csv so that Simon can access and join to habitat characteristics d_sum_albers %&gt;% readr::write_csv(file = paste0(getwd(), &#39;/data/fiss_density.csv&#39;)) Next steps?: While Joe and Nadine tweak the channel-width-21 analysis and have a look at what data from here they think we should keep or throw away Simon maybe burns dataframe into postgres that represents the X and Y locations of these sites and ties the locations of the sites to stream segments then pulls out everything he can about the watershed upstream (ex. elevation, watershed size, channel width, discharge, BEC zone, precip, etc ). Future considerations include perhaps forest cover, geology, channel confinement, Variable Infiltration Capacity (VIC-GL) model stuff) We should QA how well the sites tie to streams and find a way to filter out (or correct) bad matches if possible. Feed results back to Joe and Nadine to see what we can gleen from the data. "],["session-info.html", "5 Session Info", " 5 Session Info xfun::session_info() R version 4.0.5 (2021-03-31) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 22000), RStudio 1.4.1106 Locale: LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 LC_MONETARY=English_Canada.1252 LC_NUMERIC=C LC_TIME=English_Canada.1252 Package version: askpass_1.1 assertthat_0.2.1 backports_1.2.1 base64enc_0.1.3 BH_1.75.0.0 bit_4.0.4 bit64_4.0.5 blob_1.2.1 bookdown_0.23 broom_0.7.6 bslib_0.2.4 cachem_1.0.4 callr_3.7.0 cellranger_1.1.0 chk_0.5.1 class_7.3-18 classInt_0.4-3 cli_2.5.0 clipr_0.7.1 colorspace_2.0-0 compiler_4.0.5 cpp11_0.3.1 crayon_1.4.1 curl_4.3 data.table_1.14.0 DBI_1.1.1 dbplyr_2.1.1 digest_0.6.27 dplyr_1.0.6 dtplyr_1.1.0 e1071_1.7-6 ellipsis_0.3.1 evaluate_0.14 fansi_0.4.2 farver_2.1.0 fastmap_1.1.0 forcats_0.5.1 fs_1.5.0 gargle_1.1.0 generics_0.1.0 ggdark_0.2.1 ggplot2_3.3.3 glue_1.4.2 googledrive_1.0.1 googlesheets4_0.3.0 graphics_4.0.5 grDevices_4.0.5 grid_4.0.5 gtable_0.3.0 haven_2.3.1 highr_0.9 hms_1.0.0 htmltools_0.5.1.1 httr_1.4.2 ids_1.0.1 isoband_0.2.4 janitor_2.1.0 jquerylib_0.1.4 jsonlite_1.7.2 KernSmooth_2.23-18 knitr_1.33 labeling_0.4.2 lattice_0.20.41 lifecycle_1.0.0 lubridate_1.7.10 magrittr_2.0.1 markdown_1.1 MASS_7.3.53.1 Matrix_1.3.2 memoise_2.0.0 methods_4.0.5 mgcv_1.8.34 mime_0.11 modelr_0.1.8 munsell_0.5.0 nlme_3.1.152 openssl_1.4.4 pacman_0.5.1 pillar_1.6.0 pkgconfig_2.0.3 plogr_0.2.0 prettyunits_1.1.1 processx_3.5.2 progress_1.2.2 proxy_0.4-25 ps_1.6.0 purrr_0.3.4 R6_2.5.1 rappdirs_0.3.3 RColorBrewer_1.1.2 Rcpp_1.0.7 readr_1.4.0 readwritesqlite_0.1.2 readxl_1.3.1 rematch_1.0.1 rematch2_2.1.2 remotes_2.3.0 reprex_2.0.0 rlang_0.4.11 rmarkdown_2.10 RPostgres_1.3.2 RPostgreSQL_0.6-2 RSQLite_2.2.5 rstudioapi_0.13 rvest_1.0.0 sass_0.3.1 scales_1.1.1 selectr_0.4.2 sf_0.9-8 snakecase_0.11.0 splines_4.0.5 stats_4.0.5 stringi_1.7.3 stringr_1.4.0 sys_3.4 tibble_3.1.0 tidyr_1.1.3 tidyselect_1.1.1 tidyverse_1.3.1 tinytex_0.33 tools_4.0.5 units_0.7-1 utf8_1.2.1 utils_4.0.5 uuid_0.1.4 vctrs_0.3.7 viridisLite_0.4.0 withr_2.4.2 xfun_0.25 xml2_1.3.2 yaml_2.2.1 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
